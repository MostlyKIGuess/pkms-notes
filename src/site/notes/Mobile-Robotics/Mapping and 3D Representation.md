---
{"dg-publish":true,"permalink":"/mobile-robotics/mapping-and-3-d-representation/"}
---


# Mapping and 3D Representations


Before starting anything, a lot is taken from the one and only Cyrll Stachniss lectures. Link to the [lecture](http://ais.informatik.uni-freiburg.de/teaching/ss13/robotics/recordings/rob1-15-3dmapping.mp4) .
### Point Clouds

A **point cloud** is one of the most fundamental ways to represent a 3D environment. It is essentially a large collection of points where each point has an $(x, y, z)$ coordinate.

These points are often generated by sensors like **LIDAR**, which measures distance ($\rho$), azimuth angle ($\theta$), and elevation angle ($\phi$). These raw spherical coordinates are then converted into Cartesian coordinates $(X,Y,Z)$ to form the point cloud using the following sensor model:

![Pasted image 20250919121618.png](/img/user/Mobile-Robotics/Pasted%20image%2020250919121618.png)

$$
\begin{bmatrix} X \\ Y \\ Z \end{bmatrix} = g(\rho,\theta,\phi) = \begin{bmatrix} \rho \cos(\theta) \cos(\phi) \\ \rho \sin(\theta) \cos(\phi) \\ \rho \sin(\phi) \end{bmatrix}
$$

* *Pros*:
    * No data discretization is required, so the original measurement precision is maintained.
    * The mapped area is not limited by a predefined grid size.
* *Cons*:
    * Memory usage can be unbounded as more points are added.
    * There is no explicit representation of free or unknown space, only surfaces where the sensor light reflects are recoreded.

#### Operations on Point Clouds

The basic operations of translation and rotation can be combined. A common sequence to transform a point cloud $P_s$ from a source frame $\{s\}$ to a target frame $\{s'\}$ is: 1) translate, 2) rotate, and 3) scale.

For each point $p_s$ in the cloud, the new point $p_{s'}$ is given by:

$$ p_{s'} = S_{s's} R_{s's} (p_s - t_{s's}) $$

Where:
- $t_{s's}$ is the translation vector.
- $R_{s's}$ is the rotation matrix.
- $S_{s's}$ is a scaling matrix.


### Feature Extraction from Point Clouds

A raw point cloud is just a collection of coordinates. 
To make it useful, we often need to extract higher-level features from it, like planes, spheres, or cylinders. This process of identifying geometric primitives is a form of *segmentation*.

#### Point Cloud Accumulation

A single LIDAR scan only provides a point cloud from one perspective. To build a complete map, multiple point clouds taken from different robot poses must be combined into a single, global reference frame {G}. This process is called *accumulation*. ( Usually we take a defined global frame, or just the first frame )

If we have a point cloud $P_L$ measured in the local LIDAR frame {L}, and we know the pose (position and orientation) of the LIDAR frame with respect to the global frame, we can transform point cloud into global coordinates, $P_G$. This is done using a **homogeneous transformation matrix** ${}_{L}^{G}T$.

For each point $p_i$ in the local point cloud $P_L$, the corresponding global point $p'_i$ is calculated as:

$$ p'_i = {}_{L}^{G}T \ p_i $$

By applying this transformation to all points, we can merge multiple scans to create a dense, large-scale map of the environment.

#### Plane Fitting for Road Detection

A common task for an autonomous vehicle is to identify the road surface from a LIDAR point cloud. We can model the road as a simple plane and then fit this model to the point cloud data.

The equation of a plane in 3D is:
$$ z = a + bx + cy $$
Our goal is to find the parameters $x = [a, b, c]^T$ that best fit the measured points. ( Think of this as a linear regression problem in 3D  )

For a set of $n$ points $(x_j, y_j, z_j)$ from the point cloud, we can define the measurement error for each point as the vertical distance between the measured $z_j$ and the value predicted by our plane model:

$$ e_j = z_{predicted} - z_{measured} = (a + bx_j + cy_j) - z_j $$

We can stack all $n$ of these error equations into a single matrix form:
$$ \mathbf{e} = A\mathbf{x} - \mathbf{b} $$

Where:
$$
\mathbf{e} = \begin{bmatrix} e_1 \\ e_2 \\ \vdots \\ e_n \end{bmatrix}, \quad
A = \begin{bmatrix}
1 & x_1 & y_1 \\
1 & x_2 & y_2 \\
\vdots & \vdots & \vdots \\
1 & x_n & y_n
\end{bmatrix}, \quad
\mathbf{x} = \begin{bmatrix} a \\ b \\ c \end{bmatrix}, \quad
\mathbf{b} = \begin{bmatrix} z_1 \\ z_2 \\ \vdots \\ z_n \end{bmatrix}
$$

Now to find the best-fit parameters $\hat{\mathbf{x}}$, we use the method of *least-squares* to minimize the sum of the squared errors. The cost function is:

$$ \mathcal{L}_{LS}(\mathbf{x}) = \mathbf{e}^T \mathbf{e} = (A\mathbf{x} - \mathbf{b})^T (A\mathbf{x} - \mathbf{b}) $$

To find the minimum, we take the partial derivative with respect to $\mathbf{x}$ and set it to zero:
$$ \frac{\partial \mathcal{L}_{LS}(\mathbf{x})}{\partial \mathbf{x}} = 2A^T A \hat{\mathbf{x}} - 2A^T \mathbf{b} = 0 $$

This gives us the *normal equations*:

$$ A^T A \hat{\mathbf{x}} = A^T \mathbf{b} $$

We can then solve for the optimal parameters $\hat{\mathbf{x}}$ by using an efficient numerical solver (ideally we use this in practice ) or by calculating the pseudo-inverse:

$$ \hat{\mathbf{x}} = (A^T A)^{-1} A^T \mathbf{b} $$

This gives us the parameters $[a, b, c]$ of the plane that best represents the road surface in the point cloud.
### Voxel Grids

While point clouds are a direct representation of sensor data, they can be inefficient for tasks like collision checking.
Voxel grids address this by discretizing 3D space into a grid of volumetric elements, or **voxels**.

* *Pros*:
	- Volumetric representation of space.
	- Constant access time.
	- Allows probabilistic updates.
* *Cons*:
    * Memory requirements can be very high, as the entire volume of the map is allocated.
    * The map's extent must be known beforehand.

### 2.5D Maps / Height Maps

A common simplification of a full 3D voxel grid is a *2.5D map*, often called a height map. Instead of storing a full 3D grid, a 2.5D map is a 2D grid where each cell stores a single height value, typically representing the highest point of an object within that cell's column. 

Random Link: [[Inverse Graphics/2.5-D-Scene-Representation\|2.5-D-Scene-Representation]]
A simple way to generate this is to average all the scan points that fall into a given (x,y) grid cell.

![Pasted image 20250919145531.png](/img/user/Mobile-Robotics/Pasted%20image%2020250919145531.png)

This representation is very memory efficient and provides constant-time access, but it's non-probabilistic and cannot distinguish between free and unknown space.

#### Elevation Maps

An *elevation map* is a more sophisticated, probabilistic version of a height map. Instead of just storing an average height, each cell stores a *probabilistic estimate* of the height, often updated using a *Kalman filter*. This allows the map to represent uncertainty, which typically increases with the measured distance from the sensor.

* *Pros*:
    * Memory efficient 2.5D representation.
    * Provides a probabilistic estimate of height.
* *Cons*:
    * Cannot represent vertical objects or multiple levels (like a bridge over a road).
    * ![Pasted image 20250919150040.png](/img/user/Mobile-Robotics/Pasted%20image%2020250919150040.png)


#### Multi-Level Surface (MLS) Maps

To overcome the single-level limitation of elevation maps, *Multi-Level Surface (MLS) maps* can be used. An MLS map allows each 2D cell to store multiple surface "patches". This enables the representation of complex vertical structures like bridges and underpasses.

Each patch in a cell consists of:
- The height mean, $\mu$ 
- The height variance, $\sigma^2$ 
- A depth value, $d$ 

TODO: Add more here, conversion from LIDAR to MLS .
### Octrees (OctoMap)

While voxel grids are powerful, their memory usage is a significant drawback, especially in 3D where most cells are empty. 
The *Octree* is a hierarchical data structure that efficiently addresses this problem by only allocating memory for volumes as needed.

An Octree works by recursively subdividing 3D space. A root node represents the entire volume. If this volume is not homogeneous (i.e., not entirely occupied, free, or unknown), it is subdivided into eight children nodes ("octants"), each representing a sub-volume. This process continues until a minimum voxel size is reached or a node's volume is homogeneous.

* *Pros*:
    - It is a full 3D model that is memory efficient.
    - Can be updated probabilistically using a log-odds representation, just like occupancy grids.
    - It is inherently multi-resolution, allowing for queries at different levels of detail.
    - Open Source available [here](http://octomap.sf.net)
	
* *Cons*:
    * The implementation IS tricky.

TODO: Probabilistic map updates, Kalman Filter pre-context and Bayes pre-context for adding more content.
### Signed Distance Functions (SDF)

A *Signed Distance Function (SDF)* is way to represent geometry implicitly. An SDF is a function $f(p): \mathbb{R}^3 \to \mathbb{R}$ that, for any point $p$ in space, returns the shortest distance to a surface.

- The **sign** of the function indicates whether the point is *inside* (negative) or *outside* (positive) the surface.
- The surface itself is the set of all points where the function is zero, known as the *zero level-set*.

For example, the SDF for a sphere of radius $r$ centered at the origin is:
$$ f(p) = ||p|| - r $$

A key property is that the **gradient** of the SDF, $\nabla f(p)$, is a unit vector that always points in the direction of the closest point on the surface. This is extremely useful for optimization-based algorithms in motion planning and reconstruction.

### Occupancy Grids

An occupancy grid is a specific type of voxel grid where each cell $m_i$ stores the probability $P(m_i)$ that the corresponding region of space is occupied. This is powerful because it explicitly models uncertainty and distinguishes between *occupied*, *free*, and *unknown* space.

The map is updated sequentially using a *Bayesian filter*. Given a new measurement $z_t$, we want to update the probability of a cell $m_i$ being occupied. Using Bayes' rule, the update is:

$$ P(m_i | z_{1:t}) = \frac{P(z_t | m_i, z_{1:t-1}) P(m_i | z_{1:t-1})}{P(z_t | z_{1:t-1})} $$

This can be computationally expensive. A more efficient way to handle this is to use the **log-odds** representation. The odds of a cell being occupied are $\frac{P(m_i)}{1 - P(m_i)}$, and the log-odds are:

$$ l(m_i) = \log \frac{P(m_i)}{1 - P(m_i)} $$

The update then becomes a simple addition:

$$ l_t(m_i) = l_{t-1}(m_i) + l_{inverse\_sensor\_model} $$

Here, $l_{inverse\_sensor\_model}$ is a pre-calculated log-odds value based on the sensor model. A positive value increases the belief of occupancy (e.g., for the cell where the beam ends), and a negative value increases the belief of the cell being free (for cells the beam passes through). This avoids costly multiplications and makes the map update very fast.